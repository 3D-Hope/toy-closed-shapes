defaults:
  - scene_diffuser_base

# Note: DDIM scheduler is recommended for RL training to prevent OOM problems.
ddpo:
  incremental_training: False
  increments: [10,20,30,40,50,60,70,80,90,100,110,120,130,140,150]
  increment_type: 'constant' # constant / linear 
  training_iter_per_increment: 6000
  increment_linear_slope: 40
  joint_training: False
  training_steps_start: 0
  use_non_penetration_reward: False
  use_object_number_reward: False
  use_prompt_following_reward: False
  use_physical_feasible_objects_reward: False
  use_iou_reward: False
  use_custom_non_penetration_reward: False
  use_has_sofa_reward: False
  use_universal_reward: False  # Use composite reward with multiple physics constraints
  use_composite_reward: True  # Use composite reward combining universal + dynamic rewards
  use_inpaint: False  # Use inpainting-based diffusion for RL finetuning
  dynamic_constraint_rewards:
    use: False
    dynamic_only: False
    reward_base_dir: "/media/ajad/YourBook/AshokSaugatResearchBackup/AshokSaugatResearch/steerable-scene-generation/dynamic_constraint_rewards"
    reward_code_dir: "${algorithm.ddpo.dynamic_constraint_rewards.reward_base_dir}/${algorithm.ddpo.dynamic_constraint_rewards.user_query}_dynamic_reward_functions_final"
    user_query: "classroom"
    room_type: ${dataset.data.room_type}
    stats_path: "${algorithm.ddpo.dynamic_constraint_rewards.reward_base_dir}/${algorithm.ddpo.dynamic_constraint_rewards.user_query}_stats.json"
    weights_path: "${algorithm.ddpo.dynamic_constraint_rewards.reward_base_dir}/${algorithm.ddpo.dynamic_constraint_rewards.user_query}_responses_tmp/llm_response_4.json"
    inpaint_path: "${algorithm.ddpo.dynamic_constraint_rewards.reward_base_dir}/${algorithm.ddpo.dynamic_constraint_rewards.user_query}_responses_tmp/llm_response_3.json"
    # Overall weights for universal vs dynamic rewards
    universal_weight: 3.0  # Weight for all universal rewards combined
    dynamic_weight: 1.0    # Weight for all dynamic rewards combined
    
    # Importance weights for individual universal rewards (applied within universal_reward)
    # These are applied AFTER normalization to [0, 1] range
    
    agentic: true
    # Importance weights for individual dynamic rewards (applied within dynamic_reward)
    # These are applied AFTER normalization to [0, 1] range
    # Will be merged with any weights suggested by LLM
    # dynamic_importance_weights: dynamically generated by LLM
      # Example: kids_bed: 1.5  # 50% more important than baseline
      # LLM-generated constraints will have default weight of 1.0 unless specified
  # The RL batch size, independent of the one specified in the experiment config.
  batch_size: 32
  # Only compute policy gradients on the last n timesteps. Set to 0 to use all timesteps.
  # Note that this requires `DDPStrategy(find_unused_parameters=True)` for distributed
  # training.
  last_n_timesteps_only: 0
  # Uniformly sample this many timesteps for gradient computation. Set to 0 to use all
  # timesteps. Mutually exclusive with last_n_timesteps_only. Note that this requires
  # `DDPStrategy(find_unused_parameters=True)` for distributed training.
  n_timesteps_to_sample: 0
  advantage_max: 5.0 # Maximum advantage value
  num_reward_workers: 1
  ppo:
    num_epochs: 4 # Number of updates before sampling new data
    clip_range: 1e-4
  # The total loss is rl_loss + ddpm_reg_weight * ddpm_loss.
  # See https://arxiv.org/abs/2401.12244 for this regularization technique. Note that
  # this uses the batch size specified in the experiment config.
  ddpm_reg_weight: 500.0
  # Used when `use_physical_feasible_objects_reward` is True.
  physical_feasibility:
    non_penetration_threshold: -1.0e-3 # Allow 1mm penetration.
    use_sim: True
    sim_duration: 0.1
    sim_time_step: 1.0e-3
    sim_translation_threshold: 1.0e-3
    sim_rotation_threshold: 1.0e-2
    static_equilibrium_distance_threshold: 1.0e-3 # 1mm
  
  # Used when `use_universal_reward` is True.
  # Composite reward combines multiple physics-based rewards with importance weighting.
  universal_reward:
    use_physcene_reward: False
    # Room type for must-have furniture reward ('bedroom', 'living_room', etc.)
    room_type: 'bedroom'
    
    # Importance weights (1.0 = baseline, >1.0 = more important, <1.0 = less important)
    # These are applied AFTER normalization to [-1, 0] range.
    # All rewards are normalized first, then weighted by importance.
    importance_weights:
      universal_importance_weights:
      must_have_furniture: 1.0  # Room must be functional
      non_penetration: 1.0      # No overlapping objects
      not_out_of_bounds: 1.0    # Objects must be within room boundaries
      accessibility: 1.0        # Furniture must be accessible
      object_count: 1.0         # Realistic clutter level     # Important: realistic clutter level

# Whether to only diffuse over the continuous/ discrete part while taking the other part
# from the dataset.
continuous_discrete_only:
  continuous_only: False
  discrete_only: False


custom:
  use: true
  num_classes: 22
  