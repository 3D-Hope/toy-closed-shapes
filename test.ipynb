{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44efc7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ACTUAL DDPO BATCHING LOGIC\n",
      "================================================================================\n",
      "\n",
      "Total samples collected: 8\n",
      "Sample IDs: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\n",
      "================================================================================\n",
      "Inner Epoch 0\n",
      "================================================================================\n",
      "\n",
      "After shuffling: [5, 3, 6, 2, 1, 0, 7, 4]\n",
      "\n",
      "Number of minibatches: 4\n",
      "\n",
      "Minibatch 0: samples [5, 3]\n",
      "  Timestep 0: processing samples [5, 3]\n",
      "  Timestep 1: processing samples [5, 3]\n",
      "  Timestep 2: processing samples [5, 3]\n",
      "\n",
      "Minibatch 1: samples [6, 2]\n",
      "  Timestep 0: processing samples [6, 2]\n",
      "  Timestep 1: processing samples [6, 2]\n",
      "  Timestep 2: processing samples [6, 2]\n",
      "\n",
      "Minibatch 2: samples [1, 0]\n",
      "  Timestep 0: processing samples [1, 0]\n",
      "  Timestep 1: processing samples [1, 0]\n",
      "  Timestep 2: processing samples [1, 0]\n",
      "\n",
      "Minibatch 3: samples [7, 4]\n",
      "  Timestep 0: processing samples [7, 4]\n",
      "  Timestep 1: processing samples [7, 4]\n",
      "  Timestep 2: processing samples [7, 4]\n",
      "\n",
      "================================================================================\n",
      "KEY INSIGHT:\n",
      "================================================================================\n",
      "With num_inner_epochs = 1:\n",
      "  - Each sample appears in EXACTLY ONE minibatch\n",
      "  - Minibatch 0 has samples: [shuffled indices 0-1]\n",
      "  - Minibatch 1 has samples: [shuffled indices 2-3]\n",
      "  - Minibatch 2 has samples: [shuffled indices 4-5]\n",
      "  - Minibatch 3 has samples: [shuffled indices 6-7]\n",
      "\n",
      "Each sample is used 3 times (once per timestep)\n",
      "But each sample is in a DIFFERENT minibatch (no reuse across minibatches)\n",
      "\n",
      "================================================================================\n",
      "IF num_inner_epochs = 3:\n",
      "================================================================================\n",
      "  - Samples would be re-shuffled and re-batched 3 times\n",
      "  - Each sample would be used 9 times total\n",
      "  - THIS is where sample reuse happens!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# # Toy example to demonstrate DDPO batching logic\n",
    "# print(\"=\" * 80)\n",
    "# print(\"TOY EXAMPLE: DDPO Batching and Sample Reuse\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # Configuration (toy values)\n",
    "# sample_batch_size = 4  # samples generated per batch during sampling\n",
    "# num_batches_per_epoch = 2  # how many sampling batches per epoch\n",
    "# train_batch_size = 2  # minibatch size for training\n",
    "# num_inner_epochs = 1  # how many times to reuse samples\n",
    "# num_timesteps = 3  # number of diffusion timesteps\n",
    "# gradient_accumulation_steps = 2  # accumulate gradients over this many minibatches\n",
    "\n",
    "# # Step 1: SAMPLING PHASE\n",
    "# print(\"\\n--- SAMPLING PHASE ---\")\n",
    "# total_samples = sample_batch_size * num_batches_per_epoch\n",
    "# print(f\"Total samples collected: {total_samples}\")\n",
    "\n",
    "# # Simulate collected samples (shape: [total_samples, num_timesteps])\n",
    "# samples = {\n",
    "#     \"latents\": torch.randn(total_samples, num_timesteps, 4),  # simplified\n",
    "#     \"timesteps\": torch.randint(0, 1000, (total_samples, num_timesteps)),\n",
    "#     \"log_probs\": torch.randn(total_samples, num_timesteps),\n",
    "#     \"advantages\": torch.randn(total_samples),  # one per sample\n",
    "# }\n",
    "\n",
    "# print(f\"Sample shapes:\")\n",
    "# print(f\"  latents: {samples['latents'].shape}\")\n",
    "# print(f\"  advantages: {samples['advantages'].shape}\")\n",
    "\n",
    "# # Step 2: TRAINING PHASE\n",
    "# print(\"\\n--- TRAINING PHASE ---\")\n",
    "\n",
    "# for inner_epoch in range(num_inner_epochs):\n",
    "#     print(f\"\\nInner Epoch {inner_epoch}:\")\n",
    "    \n",
    "#     # Shuffle samples\n",
    "#     perm = torch.randperm(total_samples)\n",
    "#     samples_shuffled = {k: v[perm] for k, v in samples.items()}\n",
    "    \n",
    "#     # Rebatch for training: reshape into minibatches\n",
    "#     samples_batched = {\n",
    "#         k: v.reshape(-1, train_batch_size, *v.shape[1:])\n",
    "#         for k, v in samples_shuffled.items()\n",
    "#     }\n",
    "    \n",
    "#     num_minibatches = samples_batched[\"latents\"].shape[0]\n",
    "#     print(f\"  Number of minibatches: {num_minibatches}\")\n",
    "#     print(f\"  Each minibatch has {train_batch_size} samples\")\n",
    "    \n",
    "#     # Convert to list of dicts\n",
    "#     samples_batched_list = [\n",
    "#         {k: samples_batched[k][i] for k in samples_batched.keys()}\n",
    "#         for i in range(num_minibatches)\n",
    "#     ]\n",
    "    \n",
    "#     # Iterate over minibatches\n",
    "#     optimizer_steps = 0\n",
    "#     for i, minibatch in enumerate(samples_batched_list):\n",
    "#         print(f\"\\n  Minibatch {i}:\")\n",
    "#         print(f\"    Shape: {minibatch['latents'].shape}\")\n",
    "#         print(f\"    Samples in this minibatch: {train_batch_size}\")\n",
    "        \n",
    "#         # Iterate over timesteps\n",
    "#         for j in range(num_timesteps):\n",
    "#             # Simulate forward pass\n",
    "#             latent = minibatch['latents'][:, j]\n",
    "#             log_prob = minibatch['log_probs'][:, j]\n",
    "#             advantage = minibatch['advantages']\n",
    "            \n",
    "#             print(f\"      Timestep {j}: processing {latent.shape[0]} samples\")\n",
    "            \n",
    "#             # Simulate gradient accumulation\n",
    "#             # In real code, accelerator.sync_gradients tells us when optimizer step happens\n",
    "#             should_step = (j == num_timesteps - 1) and (\n",
    "#                 (i + 1) % gradient_accumulation_steps == 0\n",
    "#             )\n",
    "            \n",
    "#             if should_step:\n",
    "#                 optimizer_steps += 1\n",
    "#                 print(f\"      -> OPTIMIZER STEP #{optimizer_steps}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"SUMMARY:\")\n",
    "# print(\"=\" * 80)\n",
    "# print(f\"Total samples collected: {total_samples}\")\n",
    "# print(f\"Number of inner epochs: {num_inner_epochs}\")\n",
    "# print(f\"Minibatch size: {train_batch_size}\")\n",
    "# print(f\"Number of minibatches per inner epoch: {num_minibatches}\")\n",
    "# print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "# print(f\"Total optimizer steps: {optimizer_steps}\")\n",
    "# print(f\"\\nSample reuse calculation:\")\n",
    "# print(f\"  Each sample is used in: 1 minibatch × {num_timesteps} timesteps = {num_timesteps} forward passes\")\n",
    "# print(f\"  With {num_inner_epochs} inner epochs: {num_timesteps * num_inner_epochs} total forward passes per sample\")\n",
    "# print(f\"  Total forward passes: {total_samples * num_timesteps * num_inner_epochs}\")\n",
    "# print(f\"  Forward passes per optimizer step: {(total_samples * num_timesteps * num_inner_epochs) / optimizer_steps:.1f}\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# Toy example showing the CORRECT logic\n",
    "\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "total_batch_size = 8  # total samples collected (e.g., 4 batch_size × 2 num_batches_per_epoch)\n",
    "train_batch_size = 2  # minibatch size\n",
    "num_inner_epochs = 1\n",
    "num_timesteps = 3\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ACTUAL DDPO BATCHING LOGIC\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Simulate collected samples\n",
    "samples = {\n",
    "    \"latents\": torch.arange(total_batch_size).reshape(-1, 1).repeat(1, num_timesteps),\n",
    "    \"advantages\": torch.arange(total_batch_size, dtype=torch.float),\n",
    "}\n",
    "\n",
    "print(f\"\\nTotal samples collected: {total_batch_size}\")\n",
    "print(f\"Sample IDs: {samples['latents'][:, 0].tolist()}\")\n",
    "\n",
    "for inner_epoch in range(num_inner_epochs):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Inner Epoch {inner_epoch}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    # Shuffle samples\n",
    "    perm = torch.randperm(total_batch_size)\n",
    "    samples_shuffled = {k: v[perm] for k, v in samples.items()}\n",
    "    \n",
    "    print(f\"\\nAfter shuffling: {samples_shuffled['latents'][:, 0].tolist()}\")\n",
    "    \n",
    "    # Rebatch: split into minibatches\n",
    "    samples_batched = {\n",
    "        k: v.reshape(-1, train_batch_size, *v.shape[1:])\n",
    "        for k, v in samples_shuffled.items()\n",
    "    }\n",
    "    \n",
    "    num_minibatches = samples_batched[\"latents\"].shape[0]\n",
    "    print(f\"\\nNumber of minibatches: {num_minibatches}\")\n",
    "    \n",
    "    # Process each minibatch\n",
    "    for i in range(num_minibatches):\n",
    "        minibatch_sample_ids = samples_batched[\"latents\"][i, :, 0].tolist()\n",
    "        print(f\"\\nMinibatch {i}: samples {minibatch_sample_ids}\")\n",
    "        \n",
    "        # Process each timestep in this minibatch\n",
    "        for j in range(num_timesteps):\n",
    "            print(f\"  Timestep {j}: processing samples {minibatch_sample_ids}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"With num_inner_epochs = 1:\")\n",
    "print(f\"  - Each sample appears in EXACTLY ONE minibatch\")\n",
    "print(f\"  - Minibatch 0 has samples: [shuffled indices 0-1]\")\n",
    "print(f\"  - Minibatch 1 has samples: [shuffled indices 2-3]\")\n",
    "print(f\"  - Minibatch 2 has samples: [shuffled indices 4-5]\")\n",
    "print(f\"  - Minibatch 3 has samples: [shuffled indices 6-7]\")\n",
    "print(f\"\\nEach sample is used {num_timesteps} times (once per timestep)\")\n",
    "print(f\"But each sample is in a DIFFERENT minibatch (no reuse across minibatches)\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IF num_inner_epochs = 3:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  - Samples would be re-shuffled and re-batched 3 times\")\n",
    "print(f\"  - Each sample would be used {num_timesteps * 3} times total\")\n",
    "print(f\"  - THIS is where sample reuse happens!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6071b917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOY EXAMPLE: Time Dimension Shuffling and Rebatching in DDPO\n",
      "================================================================================\n",
      "\n",
      "Setup:\n",
      "  Total samples: 4\n",
      "  Number of timesteps: 3\n",
      "  Train batch size: 2\n",
      "shape of timesteps: torch.Size([4, 3])\n",
      "\n",
      "================================================================================\n",
      "ORIGINAL DATA\n",
      "================================================================================\n",
      "\n",
      "Timesteps (shape: [batch, time]):\n",
      "tensor([[1000,  800,  100],\n",
      "        [1000,  800,  200],\n",
      "        [1000,  800,  300],\n",
      "        [1000,  800,  400]])\n",
      "\n",
      "Latents (shape: [batch, time]):\n",
      "tensor([[ 10,  20,  30],\n",
      "        [110, 120, 130],\n",
      "        [210, 220, 230],\n",
      "        [310, 320, 330]])\n",
      "\n",
      "Log probs (shape: [batch, time]):\n",
      "tensor([[0.1000, 0.2000, 0.3000],\n",
      "        [1.1000, 1.2000, 1.3000],\n",
      "        [2.1000, 2.2000, 2.3000],\n",
      "        [3.1000, 3.2000, 3.3000]])\n",
      "\n",
      "Advantages (shape: [batch]):\n",
      "tensor([ 0.5000,  1.5000, -0.5000,  2.5000])\n",
      "\n",
      "================================================================================\n",
      "STEP 1: SHUFFLE ALONG BATCH DIMENSION\n",
      "================================================================================\n",
      "\n",
      "Batch permutation: [1, 2, 3, 0]\n",
      "\n",
      "Timesteps after batch shuffle:\n",
      "tensor([[1000,  800,  200],\n",
      "        [1000,  800,  300],\n",
      "        [1000,  800,  400],\n",
      "        [1000,  800,  100]])\n",
      "\n",
      "Latents after batch shuffle:\n",
      "tensor([[110, 120, 130],\n",
      "        [210, 220, 230],\n",
      "        [310, 320, 330],\n",
      "        [ 10,  20,  30]])\n",
      "\n",
      "================================================================================\n",
      "STEP 2: SHUFFLE ALONG TIME DIMENSION (INDEPENDENTLY FOR EACH SAMPLE)\n",
      "================================================================================\n",
      "shape of perms torch.Size([4, 3])\n",
      "\n",
      "Time permutations for each sample:\n",
      "  Sample 0: [1, 0, 2]\n",
      "  Sample 1: [2, 1, 0]\n",
      "  Sample 2: [2, 0, 1]\n",
      "  Sample 3: [0, 1, 2]\n",
      "shape of samples after time shuffling: torch.Size([4, 3])\n",
      "\n",
      "--- After Time Shuffling ---\n",
      "\n",
      "Timesteps (each sample's timesteps are shuffled independently):\n",
      "tensor([[ 800, 1000,  200],\n",
      "        [ 300,  800, 1000],\n",
      "        [ 400, 1000,  800],\n",
      "        [1000,  800,  100]])\n",
      "\n",
      "Latents (shuffled to match timesteps):\n",
      "tensor([[120, 110, 130],\n",
      "        [230, 220, 210],\n",
      "        [330, 310, 320],\n",
      "        [ 10,  20,  30]])\n",
      "\n",
      "Log probs (shuffled to match timesteps):\n",
      "tensor([[1.2000, 1.1000, 1.3000],\n",
      "        [2.3000, 2.2000, 2.1000],\n",
      "        [3.3000, 3.1000, 3.2000],\n",
      "        [0.1000, 0.2000, 0.3000]])\n",
      "\n",
      "** KEY INSIGHT: Each sample's timesteps are in different order **\n",
      "** This means we'll train on timesteps in random order **\n",
      "\n",
      "================================================================================\n",
      "STEP 3: REBATCH FOR TRAINING\n",
      "================================================================================\n",
      "shape of samples_batched['timesteps']: torch.Size([2, 2, 3])\n",
      "\n",
      "Number of minibatches: 2\n",
      "Each minibatch has 2 samples\n",
      "\n",
      "--- Minibatch Structure ---\n",
      "\n",
      "Minibatch 0:\n",
      "  Timesteps shape: torch.Size([2, 3])\n",
      "  Timesteps:\n",
      "tensor([[ 800, 1000,  200],\n",
      "        [ 300,  800, 1000]])\n",
      "  Latents:\n",
      "tensor([[120, 110, 130],\n",
      "        [230, 220, 210]])\n",
      "  Advantages: tensor([ 1.5000, -0.5000])\n",
      "\n",
      "Minibatch 1:\n",
      "  Timesteps shape: torch.Size([2, 3])\n",
      "  Timesteps:\n",
      "tensor([[ 400, 1000,  800],\n",
      "        [1000,  800,  100]])\n",
      "  Latents:\n",
      "tensor([[330, 310, 320],\n",
      "        [ 10,  20,  30]])\n",
      "  Advantages: tensor([2.5000, 0.5000])\n",
      "\n",
      "================================================================================\n",
      "STEP 4: CONVERT TO LIST OF DICTS (for easier iteration)\n",
      "================================================================================\n",
      "samples_batched_list [{'timesteps': tensor([[ 800, 1000,  200],\n",
      "        [ 300,  800, 1000]]), 'latents': tensor([[120, 110, 130],\n",
      "        [230, 220, 210]]), 'log_probs': tensor([[1.2000, 1.1000, 1.3000],\n",
      "        [2.3000, 2.2000, 2.1000]]), 'advantages': tensor([ 1.5000, -0.5000]), 'prompt_embeds': tensor([[[ 5.1863e-01,  1.3578e-02, -4.1196e-01,  ...,  2.0809e-01,\n",
      "           1.1545e+00,  5.2462e-01],\n",
      "         [-2.7869e-01, -4.8339e-01,  1.0137e-01,  ..., -1.6243e+00,\n",
      "           1.2182e+00,  1.3574e-01],\n",
      "         [-1.8194e-02, -1.3628e-03,  9.9495e-01,  ..., -1.2711e+00,\n",
      "          -1.2057e+00, -7.0261e-01],\n",
      "         ...,\n",
      "         [ 2.0108e+00, -7.7353e-01, -1.0223e+00,  ..., -6.7572e-01,\n",
      "           4.5511e-01,  1.5687e+00],\n",
      "         [ 1.0960e+00,  6.5780e-03,  1.9660e-01,  ..., -8.6442e-01,\n",
      "           5.1836e-01, -1.6815e+00],\n",
      "         [ 2.6809e-01, -2.5538e-01,  1.0482e+00,  ...,  9.3642e-01,\n",
      "           1.8225e+00, -3.5876e-01]],\n",
      "\n",
      "        [[-7.3800e-01,  6.5800e-01,  3.9126e-01,  ..., -3.2945e-01,\n",
      "           5.2315e-01,  1.1201e+00],\n",
      "         [-1.5402e-01, -1.9385e+00,  5.1361e-03,  ...,  6.7344e-01,\n",
      "          -2.6512e-01,  6.2066e-01],\n",
      "         [ 4.7914e-01, -1.2445e+00,  2.0961e-01,  ...,  6.1378e-01,\n",
      "          -9.0177e-01, -5.0653e-01],\n",
      "         ...,\n",
      "         [ 7.8862e-01, -2.0465e+00, -1.1183e+00,  ...,  6.5417e-01,\n",
      "          -1.6418e+00, -6.2948e-01],\n",
      "         [-8.5465e-01,  8.4438e-01, -1.8896e+00,  ...,  8.8981e-01,\n",
      "          -1.5828e+00, -2.1964e-01],\n",
      "         [-2.1901e-01, -6.0068e-01,  4.1984e-02,  ..., -7.7260e-01,\n",
      "           8.3725e-01, -1.2780e+00]]])}, {'timesteps': tensor([[ 400, 1000,  800],\n",
      "        [1000,  800,  100]]), 'latents': tensor([[330, 310, 320],\n",
      "        [ 10,  20,  30]]), 'log_probs': tensor([[3.3000, 3.1000, 3.2000],\n",
      "        [0.1000, 0.2000, 0.3000]]), 'advantages': tensor([2.5000, 0.5000]), 'prompt_embeds': tensor([[[-0.9349,  0.8159, -0.5640,  ..., -2.3434,  0.1397,  0.2308],\n",
      "         [-0.6605,  0.9051, -0.1408,  ..., -0.6752,  0.4713, -0.0615],\n",
      "         [ 0.3230, -0.0253,  0.6429,  ..., -0.0177, -0.6139,  0.3782],\n",
      "         ...,\n",
      "         [-1.0595, -0.9435, -0.6309,  ...,  1.5886,  1.0642, -0.8499],\n",
      "         [ 0.3189,  2.2983, -0.2780,  ..., -0.7556, -1.2686, -1.0714],\n",
      "         [-0.7001, -0.8128, -0.1815,  ...,  0.3102,  1.3155,  0.6196]],\n",
      "\n",
      "        [[-1.7359, -1.0534,  0.1585,  ...,  0.7217, -0.9323, -1.7623],\n",
      "         [-1.1563,  0.3546, -1.0314,  ..., -1.2566, -1.0152, -0.6350],\n",
      "         [ 0.1344,  0.8476, -0.6234,  ...,  0.1417,  0.9050,  1.3452],\n",
      "         ...,\n",
      "         [ 0.6221,  0.6555, -0.4340,  ...,  1.1976,  0.4505, -0.3506],\n",
      "         [ 2.0442, -0.5487, -2.1102,  ...,  0.5087,  0.4466,  2.0101],\n",
      "         [-1.6441,  0.2376, -2.0958,  ..., -0.9468,  0.1420,  0.3790]]])}]\n",
      "\n",
      "Number of minibatch dicts: 2\n",
      "\n",
      "================================================================================\n",
      "STEP 5: TRAINING LOOP SIMULATION\n",
      "================================================================================\n",
      "\n",
      "--- Processing Minibatch 0 ---\n",
      "Batch size: 2\n",
      "Timesteps in this minibatch:\n",
      "tensor([[ 800, 1000,  200],\n",
      "        [ 300,  800, 1000]])\n",
      "\n",
      "  Timestep position 0:\n",
      "    Timestep values: [800, 300]\n",
      "    Latents: [120, 230]\n",
      "    Log probs: [1.2000000476837158, 2.299999952316284]\n",
      "    Advantages: [1.5, -0.5]\n",
      "    -> Forward pass with UNet using these values\n",
      "    -> Compute loss and accumulate gradients\n",
      "\n",
      "  Timestep position 1:\n",
      "    Timestep values: [1000, 800]\n",
      "    Latents: [110, 220]\n",
      "    Log probs: [1.100000023841858, 2.200000047683716]\n",
      "    Advantages: [1.5, -0.5]\n",
      "    -> Forward pass with UNet using these values\n",
      "    -> Compute loss and accumulate gradients\n",
      "\n",
      "  Timestep position 2:\n",
      "    Timestep values: [200, 1000]\n",
      "    Latents: [130, 210]\n",
      "    Log probs: [1.2999999523162842, 2.0999999046325684]\n",
      "    Advantages: [1.5, -0.5]\n",
      "    -> Forward pass with UNet using these values\n",
      "    -> Compute loss and accumulate gradients\n",
      "\n",
      "--- Processing Minibatch 1 ---\n",
      "Batch size: 2\n",
      "Timesteps in this minibatch:\n",
      "tensor([[ 400, 1000,  800],\n",
      "        [1000,  800,  100]])\n",
      "\n",
      "  Timestep position 0:\n",
      "    Timestep values: [400, 1000]\n",
      "    Latents: [330, 10]\n",
      "    Log probs: [3.299999952316284, 0.10000000149011612]\n",
      "    Advantages: [2.5, 0.5]\n",
      "    -> Forward pass with UNet using these values\n",
      "    -> Compute loss and accumulate gradients\n",
      "\n",
      "  Timestep position 1:\n",
      "    Timestep values: [1000, 800]\n",
      "    Latents: [310, 20]\n",
      "    Log probs: [3.0999999046325684, 0.20000000298023224]\n",
      "    Advantages: [2.5, 0.5]\n",
      "    -> Forward pass with UNet using these values\n",
      "    -> Compute loss and accumulate gradients\n",
      "\n",
      "  Timestep position 2:\n",
      "    Timestep values: [800, 100]\n",
      "    Latents: [320, 30]\n",
      "    Log probs: [3.200000047683716, 0.30000001192092896]\n",
      "    Advantages: [2.5, 0.5]\n",
      "    -> Forward pass with UNet using these values\n",
      "    -> Compute loss and accumulate gradients\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: WHY THIS DESIGN?\n",
      "================================================================================\n",
      "\n",
      "1. BATCH SHUFFLE: Ensures different samples are grouped together in each epoch\n",
      "   - Reduces correlation between samples in a minibatch\n",
      "\n",
      "2. TIME SHUFFLE: Each sample's timesteps are processed in random order\n",
      "   - Sample 0 might process: t=600, then t=1000, then t=800\n",
      "   - Sample 1 might process: t=800, then t=600, then t=1000\n",
      "   - This breaks temporal correlation in training\n",
      "\n",
      "3. REBATCHING: Split samples into minibatches of size train_batch_size\n",
      "   - Allows gradient accumulation across multiple minibatches\n",
      "   - Each minibatch processes all its timesteps before moving to next minibatch\n",
      "\n",
      "4. TRAINING: For each minibatch, loop through all timesteps\n",
      "   - Process timestep 0 for all samples in minibatch\n",
      "   - Process timestep 1 for all samples in minibatch\n",
      "   - Process timestep 2 for all samples in minibatch\n",
      "   - Accumulate gradients across timesteps within minibatch\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOY EXAMPLE: Time Dimension Shuffling and Rebatching in DDPO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration\n",
    "total_batch_size = 4  # total samples collected\n",
    "num_timesteps = 3  # number of diffusion timesteps\n",
    "train_batch_size = 2  # minibatch size for training\n",
    "\n",
    "print(f\"\\nSetup:\")\n",
    "print(f\"  Total samples: {total_batch_size}\")\n",
    "print(f\"  Number of timesteps: {num_timesteps}\")\n",
    "print(f\"  Train batch size: {train_batch_size}\")\n",
    "\n",
    "# Create sample data with easy-to-track values\n",
    "# Shape: (total_batch_size, num_timesteps)\n",
    "timesteps = torch.tensor([\n",
    "    [1000, 800, 100],  # Sample 0\n",
    "    [1000, 800, 200],  # Sample 1\n",
    "    [1000, 800, 300],  # Sample 2\n",
    "    [1000, 800, 400],  # Sample 3\n",
    "])\n",
    "\n",
    "print(f\"shape of timesteps: {timesteps.shape}\")\n",
    "# Latents - use sample_id * 100 + timestep_position to track\n",
    "latents = torch.tensor([\n",
    "    [10, 20, 30],  # Sample 0: latent at t=1000, t=800, t=600\n",
    "    [110, 120, 130],  # Sample 1\n",
    "    [210, 220, 230],  # Sample 2\n",
    "    [310, 320, 330],  # Sample 3\n",
    "])\n",
    "\n",
    "log_probs = torch.tensor([\n",
    "    [0.1, 0.2, 0.3],  # Sample 0\n",
    "    [1.1, 1.2, 1.3],  # Sample 1\n",
    "    [2.1, 2.2, 2.3],  # Sample 2\n",
    "    [3.1, 3.2, 3.3],  # Sample 3\n",
    "])\n",
    "\n",
    "advantages = torch.tensor([0.5, 1.5, -0.5, 2.5])  # One per sample\n",
    "prompt_embeds = torch.randn(total_batch_size, 77, 768)  # Dummy embeddings\n",
    "\n",
    "samples = {\n",
    "    \"timesteps\": timesteps,\n",
    "    \"latents\": latents,\n",
    "    \"log_probs\": log_probs,\n",
    "    \"advantages\": advantages,\n",
    "    \"prompt_embeds\": prompt_embeds,\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ORIGINAL DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nTimesteps (shape: [batch, time]):\")\n",
    "print(samples[\"timesteps\"])\n",
    "print(\"\\nLatents (shape: [batch, time]):\")\n",
    "print(samples[\"latents\"])\n",
    "print(\"\\nLog probs (shape: [batch, time]):\")\n",
    "print(samples[\"log_probs\"])\n",
    "print(\"\\nAdvantages (shape: [batch]):\")\n",
    "print(samples[\"advantages\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: SHUFFLE ALONG BATCH DIMENSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Shuffle samples along batch dimension\n",
    "perm = torch.randperm(total_batch_size)\n",
    "print(f\"\\nBatch permutation: {perm.tolist()}\")\n",
    "\n",
    "samples = {k: v[perm] for k, v in samples.items()}\n",
    "\n",
    "print(\"\\nTimesteps after batch shuffle:\")\n",
    "print(samples[\"timesteps\"])\n",
    "print(\"\\nLatents after batch shuffle:\")\n",
    "print(samples[\"latents\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: SHUFFLE ALONG TIME DIMENSION (INDEPENDENTLY FOR EACH SAMPLE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate independent permutations for each sample's timesteps\n",
    "perms = torch.stack(\n",
    "    [torch.randperm(num_timesteps) for _ in range(total_batch_size)]\n",
    ")\n",
    "print(f\"shape of perms {perms.shape}\")\n",
    "print(f\"\\nTime permutations for each sample:\")\n",
    "for i in range(total_batch_size):\n",
    "    print(f\"  Sample {i}: {perms[i].tolist()}\")\n",
    "\n",
    "# Apply time shuffling to time-dependent keys\n",
    "for key in [\"timesteps\", \"latents\", \"log_probs\"]:\n",
    "    samples[key] = samples[key][\n",
    "        torch.arange(total_batch_size)[:, None],\n",
    "        perms,\n",
    "    ]\n",
    "\n",
    "print(f\"shape of samples after time shuffling: {samples['timesteps'].shape}\")\n",
    "print(\"\\n--- After Time Shuffling ---\")\n",
    "print(\"\\nTimesteps (each sample's timesteps are shuffled independently):\")\n",
    "print(samples[\"timesteps\"])\n",
    "print(\"\\nLatents (shuffled to match timesteps):\")\n",
    "print(samples[\"latents\"])\n",
    "print(\"\\nLog probs (shuffled to match timesteps):\")\n",
    "print(samples[\"log_probs\"])\n",
    "\n",
    "print(\"\\n** KEY INSIGHT: Each sample's timesteps are in different order **\")\n",
    "print(\"** This means we'll train on timesteps in random order **\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: REBATCH FOR TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Reshape to create minibatches\n",
    "samples_batched = {\n",
    "    k: v.reshape(-1, train_batch_size, *v.shape[1:])\n",
    "    for k, v in samples.items()\n",
    "}\n",
    "print(f\"shape of samples_batched['timesteps']: {samples_batched['timesteps'].shape}\")\n",
    "\n",
    "num_minibatches = samples_batched[\"timesteps\"].shape[0]\n",
    "print(f\"\\nNumber of minibatches: {num_minibatches}\")\n",
    "print(f\"Each minibatch has {train_batch_size} samples\")\n",
    "\n",
    "print(\"\\n--- Minibatch Structure ---\")\n",
    "for i in range(num_minibatches):\n",
    "    print(f\"\\nMinibatch {i}:\")\n",
    "    print(f\"  Timesteps shape: {samples_batched['timesteps'][i].shape}\")\n",
    "    print(f\"  Timesteps:\\n{samples_batched['timesteps'][i]}\")\n",
    "    print(f\"  Latents:\\n{samples_batched['latents'][i]}\")\n",
    "    print(f\"  Advantages: {samples_batched['advantages'][i]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: CONVERT TO LIST OF DICTS (for easier iteration)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert dict of tensors -> list of dicts\n",
    "samples_batched_list = [\n",
    "    dict(zip(samples_batched, x)) for x in zip(*samples_batched.values())\n",
    "]\n",
    "print(f\"samples_batched_list {samples_batched_list}\")\n",
    "print(f\"\\nNumber of minibatch dicts: {len(samples_batched_list)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: TRAINING LOOP SIMULATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, minibatch in enumerate(samples_batched_list):\n",
    "    print(f\"\\n--- Processing Minibatch {i} ---\")\n",
    "    print(f\"Batch size: {minibatch['timesteps'].shape[0]}\")\n",
    "    print(f\"Timesteps in this minibatch:\\n{minibatch['timesteps']}\")\n",
    "    \n",
    "    # Loop over timesteps\n",
    "    for j in range(num_timesteps):\n",
    "        print(f\"\\n  Timestep position {j}:\")\n",
    "        print(f\"    Timestep values: {minibatch['timesteps'][:, j].tolist()}\")\n",
    "        print(f\"    Latents: {minibatch['latents'][:, j].tolist()}\")\n",
    "        print(f\"    Log probs: {minibatch['log_probs'][:, j].tolist()}\")\n",
    "        print(f\"    Advantages: {minibatch['advantages'].tolist()}\")\n",
    "        print(f\"    -> Forward pass with UNet using these values\")\n",
    "        print(f\"    -> Compute loss and accumulate gradients\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY: WHY THIS DESIGN?\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. BATCH SHUFFLE: Ensures different samples are grouped together in each epoch\n",
    "   - Reduces correlation between samples in a minibatch\n",
    "\n",
    "2. TIME SHUFFLE: Each sample's timesteps are processed in random order\n",
    "   - Sample 0 might process: t=600, then t=1000, then t=800\n",
    "   - Sample 1 might process: t=800, then t=600, then t=1000\n",
    "   - This breaks temporal correlation in training\n",
    "\n",
    "3. REBATCHING: Split samples into minibatches of size train_batch_size\n",
    "   - Allows gradient accumulation across multiple minibatches\n",
    "   - Each minibatch processes all its timesteps before moving to next minibatch\n",
    "\n",
    "4. TRAINING: For each minibatch, loop through all timesteps\n",
    "   - Process timestep 0 for all samples in minibatch\n",
    "   - Process timestep 1 for all samples in minibatch\n",
    "   - Process timestep 2 for all samples in minibatch\n",
    "   - Accumulate gradients across timesteps within minibatch\n",
    "\"\"\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steerable-scene-generation-py3.10 (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
